{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_raw.pickle', 'rb') as handle:\n",
    "    x_raw = pickle.load(handle)\n",
    "with open('label.pickle', 'rb') as handle:\n",
    "    label = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing option 1: finding the min # of samples and downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "min_sample_size = min([m.shape[0] for m in x_raw])\n",
    "print(min_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_downsampled = []\n",
    "for elem in x_raw:\n",
    "    # randomly sampling the indices for each matrix\n",
    "    random_idx = np.random.randint(0, elem.shape[0], min_sample_size)\n",
    "    # sort the indices to match the timestamps\n",
    "    random_idx.sort()\n",
    "    # downsampling\n",
    "    x_downsampled.append(elem[random_idx, :])\n",
    "x_downsampled = np.array(x_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 78, 3)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_downsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = x_downsampled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffling and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 234)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(x_downsampled).reshape(sample_size, -1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.eye(num_classes)[np.array(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 10)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 244)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tmp = np.hstack([X, y])\n",
    "data_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 244)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the data\n",
    "np.random.seed(11)\n",
    "np.random.shuffle(data_tmp)\n",
    "data_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tmp[0:5, -10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 234) (126, 10)\n"
     ]
    }
   ],
   "source": [
    "X = data_tmp[:, :-10]\n",
    "y = data_tmp[:, -10:]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 234) (46, 234)\n",
      "(80, 10) (46, 10)\n"
     ]
    }
   ],
   "source": [
    "train_idx = 80\n",
    "X_train, X_test = X[:train_idx, :], X[train_idx:, :]\n",
    "y_train, y_test = y[:train_idx, :], y[train_idx:, :]\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, max_depth=5)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2391304347826087"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = LogisticRegression(penalty='l2', C=1.0, solver='newton-cg', multi_class='multinomial')\n",
    "lg.fit(X_train, np.dot(y_train, range(num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_train, np.dot(y_train, range(num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7391304347826086"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_test, np.dot(y_test, range(num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.dot(y_test, range(num_classes)), \n",
    "                      lg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 5, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 4, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 4, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 2, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 2, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 7, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 3, 0],\n",
       "       [0, 2, 0, 0, 1, 0, 0, 1, 0, 2]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=3, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=1.0, kernel='poly', degree=3, decision_function_shape='ovo')\n",
    "svc.fit(X_train, np.dot(y_train, range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_train, np.dot(y_train, range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test, np.dot(y_test, range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =  X_train.shape[1]\n",
    "fc1_shape = 64\n",
    "fc2_shape = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, input_shape], name='x')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_')\n",
    "keep_proba = tf.placeholder(tf.float32, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fc1/weights:0' shape=(234, 64) dtype=float32_ref>\n",
      "Tensor(\"fc1/dropout/mul_1:0\", shape=(?, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"fc1\") as scope:\n",
    "    weights = tf.get_variable(name=\"weights\", shape=[input_shape, fc1_shape], dtype=tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "    bias = tf.get_variable(name=\"bias\", shape=[fc1_shape], dtype=tf.float32,\n",
    "                           initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "    fc = tf.matmul(x, weights, name=\"matmul\")\n",
    "    pre_activation = tf.add(fc, bias)\n",
    "    fc1 = tf.nn.relu(pre_activation, name=\"relu\")\n",
    "    dp1 = tf.nn.dropout(fc1, keep_prob=keep_proba)\n",
    "\n",
    "print(weights)\n",
    "print(dp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fc2/weights:0' shape=(64, 32) dtype=float32_ref>\n",
      "Tensor(\"fc2/relu:0\", shape=(?, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"fc2\") as scope:\n",
    "    weights = tf.get_variable(name=\"weights\", shape=[fc1_shape, fc2_shape], dtype=tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "    bias = tf.get_variable(name=\"bias\", shape=[fc2_shape], dtype=tf.float32,\n",
    "                           initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "    fc = tf.matmul(dp1, weights, name=\"matmul\")\n",
    "    pre_activation = tf.add(fc, bias)\n",
    "    fc2 = tf.nn.relu(pre_activation, name=\"relu\")\n",
    "    dp2 = tf.nn.dropout(fc2, keep_prob=keep_proba)\n",
    "\n",
    "print(weights)\n",
    "print(fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fc3/weights:0' shape=(32, 10) dtype=float32_ref>\n",
      "Tensor(\"fc3/logits:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"fc3\") as scope:\n",
    "    weights = tf.get_variable(name=\"weights\", shape=[fc2_shape, num_classes], dtype=tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "    bias = tf.get_variable(name=\"bias\", shape=[num_classes], dtype=tf.float32,\n",
    "                           initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "    fc = tf.matmul(dp2, weights, name=\"matmul\")\n",
    "    logits = tf.add(fc, bias, name=\"logits\")\n",
    "    y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "    y_pred = tf.argmax(logits, axis=1, name=\"y_pred\")\n",
    "\n",
    "print(weights)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loss/cross_entropy/Reshape_2:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"loss/cross_entropy_loss:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits, name=\"cross_entropy\")\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"cross_entropy_loss\")\n",
    "    #train_step = tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(loss, name=\"train_step\")\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss, name=\"train_step\")\n",
    "print(cross_entropy)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"eval/Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"eval/accuracy:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.equal(y_pred, tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "print(correct)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 8.80593, training accuracy: 0.15\n",
      "Epoch 0, validation loss: 6.92341, validation accuracy 0.173913\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = sess.run([loss, accuracy], \n",
    "                                      feed_dict={x: X_train, \n",
    "                                                 y_: y_train,\n",
    "                                                 keep_proba: 1.0})\n",
    "print('Epoch %d, training loss: %g, training accuracy: %g' % (0, train_loss, train_accuracy))\n",
    "val_loss, val_accuracy = sess.run([loss, accuracy], \n",
    "                                  feed_dict={x: X_test,\n",
    "                                             y_: y_test,\n",
    "                                             keep_proba: 1.0})\n",
    "print('Epoch %d, validation loss: %g, validation accuracy %g' % (0, val_loss, val_accuracy))\n",
    "#results[\"init_loss\"].append(val_loss)\n",
    "#results[\"init_acc\"].append(val_accuracy)\n",
    "max_accuracy = val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_epochs = 50\n",
    "n_batches = int(X_train.shape[0] / batch_size)\n",
    "display_step = 5\n",
    "max_accuracy = 0\n",
    "\n",
    "save_ckps_dir = \"./chkps_mnist_air/dnn2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, training loss: 0.00494452, training accuracy: 1\n",
      "Epoch 4, validation loss: 1.23186, validation accuracy 0.673913\n",
      "Epoch 9, training loss: 0.00456187, training accuracy: 1\n",
      "Epoch 9, validation loss: 1.2031, validation accuracy 0.630435\n",
      "Epoch 14, training loss: 0.00446191, training accuracy: 1\n",
      "Epoch 14, validation loss: 1.22853, validation accuracy 0.630435\n",
      "Epoch 19, training loss: 0.00418288, training accuracy: 1\n",
      "Epoch 19, validation loss: 1.24122, validation accuracy 0.630435\n",
      "Epoch 24, training loss: 0.00400958, training accuracy: 1\n",
      "Epoch 24, validation loss: 1.23749, validation accuracy 0.652174\n",
      "Epoch 29, training loss: 0.00408561, training accuracy: 1\n",
      "Epoch 29, validation loss: 1.21842, validation accuracy 0.630435\n",
      "Epoch 34, training loss: 0.00387001, training accuracy: 1\n",
      "Epoch 34, validation loss: 1.21526, validation accuracy 0.652174\n",
      "Epoch 39, training loss: 0.0040474, training accuracy: 1\n",
      "Epoch 39, validation loss: 1.24065, validation accuracy 0.630435\n",
      "Epoch 44, training loss: 0.00352922, training accuracy: 1\n",
      "Epoch 44, validation loss: 1.24944, validation accuracy 0.630435\n",
      "Epoch 49, training loss: 0.00328551, training accuracy: 1\n",
      "Epoch 49, validation loss: 1.24095, validation accuracy 0.652174\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "    batch_start = 0\n",
    "    batch_end = batch_size\n",
    "    for j in range(n_batches):\n",
    "        batch_images, batch_labels = X_train[batch_start:batch_end, :], y_train[batch_start:batch_end, :]\n",
    "        sess.run(train_step, feed_dict={x: batch_images, \n",
    "                                        y_: batch_labels,\n",
    "                                        keep_proba: 0.8})\n",
    "        batch_start += batch_size\n",
    "        batch_end += batch_size\n",
    "        \n",
    "    train_loss, train_accuracy = sess.run([loss, accuracy], \n",
    "                                          feed_dict={x: X_train, \n",
    "                                                     y_: y_train,\n",
    "                                                     keep_proba: 1.0})\n",
    "    #results[\"train_loss\"].append(train_loss)\n",
    "    #results[\"train_acc\"].append(train_accuracy)\n",
    "\n",
    "    val_loss, val_accuracy = sess.run([loss, accuracy], \n",
    "                                      feed_dict={x: X_test,\n",
    "                                                 y_: y_test, \n",
    "                                                 keep_proba: 1.0})\n",
    "    #results[\"val_loss\"].append(val_loss)\n",
    "    #results[\"val_acc\"].append(val_accuracy)\n",
    "    \n",
    "    if val_accuracy > max_accuracy:\n",
    "        print(\"Save the current model!\")\n",
    "        max_accuracy = val_accuracy\n",
    "        saver.save(sess, save_ckps_dir)\n",
    "    \n",
    "    if (i+1) % display_step == 0:\n",
    "        print('Epoch %d, training loss: %g, training accuracy: %g' % (i, train_loss, train_accuracy))\n",
    "        print('Epoch %d, validation loss: %g, validation accuracy %g' % (i, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.saved_model import signature_def_utils, signature_constants, tag_constants\n",
    "from tensorflow.saved_model import utils as save_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "savemodel_file_path = \"./savedmodel/mnist_air_dnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_op = sess.graph.get_operation_by_name('x')\n",
    "x_node = x_op.outputs[0]\n",
    "pred_op = sess.graph.get_operation_by_name('fc3/y_proba')\n",
    "pred = pred_op.outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1114 17:29:00.628089 4668990912 deprecation.py:323] From <ipython-input-52-9cbaaab7d65e>:2: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    }
   ],
   "source": [
    "prediction_signature = signature_def_utils.build_signature_def(\n",
    "    inputs={\"input\": save_model_utils.build_tensor_info(x)},\n",
    "    outputs={\"output\":save_model_utils.build_tensor_info(pred)},\n",
    "    method_name=signature_constants.PREDICT_METHOD_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'./savedmodel/mnist_air_dnn/saved_model.pb'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = tf.saved_model.builder.SavedModelBuilder(savemodel_file_path)\n",
    "builder.add_meta_graph_and_variables(sess, [tag_constants.SERVING], \n",
    "                                     signature_def_map={\"predict\": prediction_signature})\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu]",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
